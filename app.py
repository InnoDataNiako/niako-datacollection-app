import streamlit as st
import os
import pandas as pd
from datetime import datetime
from scraping.scraper_4url_BS import scrape_category
import unicodedata
import plotly.express as px
import tempfile

# Partie 2: Configuration de la page et du style
st.set_page_config(page_title="CoinAfrique", layout="wide")

# Partie 3: CSS pour le style de la sidebar et des cartes
st.markdown("""
    <style>
    /* Fond noir pour la sidebar */
    [data-testid="stSidebar"] {
        background-color: #111827;
        color: #ffffff;
    }
    .sidebar-title {
        font-size: 22px;
        font-weight: bold;
        color: #ffffff;
        text-align: center;
        padding: 20px 10px 10px 10px;
        border-bottom: 1px solid #374151;
    }
    .search-box {
        margin-top: 10px;
        margin-bottom: 20px;
        padding: 0 10px;
    }
    div[role="radiogroup"] > label {
        color: #f3f4f6;
        padding: 10px;
        border-radius: 8px;
        margin: 4px 10px;
        display: flex;
        align-items: center;
        font-size: 16px;
        font-weight: 500;
    }
    div[role="radiogroup"] > label:hover {
        background-color: #1f2937;
        cursor: pointer;
    }
    hr {
        border: none;
        border-top: 1px solid #374151;
        margin: 10px 10px;
    }
    .centered-title {
        text-align: center;
        margin-bottom: 1.5rem;
    }
    </style>
""", unsafe_allow_html=True)

# Partie 4: Titre de la sidebar et menu principal
st.sidebar.markdown('<div class="sidebar-title">ğŸ“¦ CoinAfrique</div>', unsafe_allow_html=True)
menu = st.sidebar.radio("FonctionnalitÃ© Principale", [
    "ğŸ”„ Scraper les donnÃ©es",
    "ğŸ“¥ TÃ©lÃ©charger WebScraper",
    "ğŸ“Š Dashboard DonnÃ©es NettoyÃ©es",
    "ğŸ“ Formulaire d'Ã©valuation",
])

# Partie 5: Affichage du titre principal en fonction du menu sÃ©lectionnÃ©
if menu == "ğŸ”„ Scraper les donnÃ©es":
    st.markdown("<h1 class='centered-title'>Scraper avec BeautifulSoup</h1>", unsafe_allow_html=True)
elif menu == "ğŸ“¥ TÃ©lÃ©charger WebScraper":
    st.markdown("""
    <div class="download-card">
        <div class="download-header">
            <div class="download-icon">ğŸ“¥</div>
            <h2 class="download-title">TÃ©lÃ©charger les donnÃ©es WebScraper</h2>
        </div>
        <p style="color: #6b7280; margin-bottom: 20px;">
            SÃ©lectionnez les fichiers CSV Ã  tÃ©lÃ©charger. Ces fichiers contiennent les donnÃ©es dÃ©jÃ  scrapÃ©es (non nettoyÃ©es).
        </p>
    </div>
    """, unsafe_allow_html=True)
elif menu == "ğŸ“Š Dashboard DonnÃ©es NettoyÃ©es":
    st.markdown("<h1 class='centered-title'>Dashboard DonnÃ©es NettoyÃ©es</h1>", unsafe_allow_html=True)
elif menu == "ğŸ“ Formulaire d'Ã©valuation":
    st.markdown('<div class="form-title">ğŸ“ Formulaire d\'Ã‰valuation</div>', unsafe_allow_html=True)

# Partie 6: Scraping avec BeautifulSoup
if menu == "ğŸ”„ Scraper les donnÃ©es":
    st.markdown("""
    <div class="scraping-card" style="margin-top: 20px;">
        <p>Choisissez une catÃ©gorie et le nombre de pages Ã  scraper sur <a href="https://sn.coinafrique.com/" target="_blank">coinafrique.com</a></p>
    </div>
    """, unsafe_allow_html=True)

    with st.container():
        st.markdown('<div class="scraping-card">', unsafe_allow_html=True)
        
        categories = {
            "VÃªtements Homme": {
                "url": "https://sn.coinafrique.com/categorie/vetements-homme",
                "column_map": {
                    "V1": "type_habits",
                    "V2": "prix",
                    "V3": "adresse",
                    "V4": "image_lien"
                },
                "icon": "ğŸ‘”" 
            },  
            "Chaussures Homme": {
                "url": "https://sn.coinafrique.com/categorie/chaussures-homme",
                "column_map": {
                    "V1": "type_chaussures",
                    "V2": "prix",
                    "V3": "adresse",
                    "V4": "image_lien"
                },
                "icon": "ğŸ‘Ÿ"
            },
            "VÃªtements Enfants": {
                "url": "https://sn.coinafrique.com/categorie/vetements-enfants",
                "column_map": {
                    "V1": "type_habits",
                    "V2": "prix",
                    "V3": "adresse",
                    "V4": "image_lien"
                },
                "icon": "ğŸ‘•"
            },
            "Chaussures Enfants": {
                "url": "https://sn.coinafrique.com/categorie/chaussures-enfants",
                "column_map": {
                    "V1": "type_chaussures_enfants",
                    "V2": "prix",
                    "V3": "adresse",
                    "V4": "image_lien"
                },
                "icon": "ğŸ‘Ÿ"
            }
        }   

        category = st.selectbox("SÃ©lectionnez une catÃ©gorie", list(categories.keys()), format_func=lambda x: f"{categories[x]['icon']} {x}")
        num_pages = st.number_input("Nombre de pages Ã  scraper", min_value=1, max_value=100, value=5, step=1)
        st.markdown('</div>', unsafe_allow_html=True)
        # Partie 9 : Bouton pour lancer le scraping
        if st.button("ğŸš€ Lancer le scraping", key="scrape_button"):
            with st.spinner("ğŸ”„ Scraping en cours... Cette opÃ©ration peut prendre quelques minutes."):
                try:
                    def slugify(text):
                        text = unicodedata.normalize("NFKD", text).encode("ascii", "ignore").decode("ascii")
                        return text.lower().replace(" ", "_").replace("&", "et")

                    filename_base = slugify(category)
                    csv_filename = f"{filename_base}_{num_pages}-pages_{datetime.now().strftime('%Y%m%d_%H%M')}.csv"

                    # CrÃ©er un fichier temporaire dans le mÃªme processus
                    tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".csv")
                    file_path = tmp_file.name
                    tmp_file.close()  # important pour qu'on puisse l'ouvrir plus tard

                    config = categories[category]
                    df = scrape_category(config["url"], file_path, config["column_map"], max_pages=num_pages)

                    if df is not None and os.path.exists(file_path):
                        st.markdown(f"""
                        <div class="scraping-card stSuccess">
                            <div class="section-title">âœ… Scraping terminÃ© avec succÃ¨s</div>
                            <p>DonnÃ©es extraites pour la catÃ©gorie <strong>{category}</strong> sur <strong>{num_pages}</strong> pages.</p>
                            <p>Fichier gÃ©nÃ©rÃ© : <code>{file_path}</code></p>
                        </div>
                        """, unsafe_allow_html=True)

                        with open(file_path, "rb") as file:
                            st.download_button(
                                label=f"ğŸ“¥ TÃ©lÃ©charger {csv_filename}",
                                data=file,
                                file_name=csv_filename,
                                mime="text/csv",
                                key="download_csv"
                            )
                        # Facultatif : supprimer le fichier aprÃ¨s
                        # os.remove(file_path)
                    else:
                        st.markdown("""
                        <div class="scraping-card stError">
                            <div class="section-title">âŒ Erreur</div>
                            <p>Le fichier scrapÃ© n'a pas pu Ãªtre trouvÃ© ou est vide.</p>
                        </div>
                        """, unsafe_allow_html=True)

                except Exception as e:
                    st.markdown(f"""
                    <div class="scraping-card stError">
                        <div class="section-title">âŒ Erreur pendant le scraping</div>
                        <p>Une erreur s'est produite :</p>
                        <code>{str(e)}</code>
                    </div>
                    """, unsafe_allow_html=True)
    